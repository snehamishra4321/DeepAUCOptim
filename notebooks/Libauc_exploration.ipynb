{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LibAUC Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libauc.models import resnet18\n",
    "from libauc.losses import AUCMLoss\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torch_data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "\n",
    "\n",
    "from libauc.losses import AUCMLoss, CrossEntropyLoss\n",
    "from libauc.optimizers import PESG, Adam\n",
    "from libauc.models import densenet121 as DenseNet121\n",
    "from libauc.datasets import CheXpert\n",
    "\n",
    "import torch \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_flag = 'pathmnist'\n",
    "data_flag = 'breastmnist'\n",
    "download = True\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "lr = 0.001\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'binary-class'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\Hari\\.medmnist\\breastmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Hari\\.medmnist\\breastmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Hari\\.medmnist\\breastmnist.npz\n"
     ]
    }
   ],
   "source": [
    "# # preprocessing\n",
    "# data_transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     # transforms.Grayscale(3),\n",
    "#     transforms.Resize((28, 28)), \n",
    "#     transforms.Normalize(mean=[.5], std=[.5])\n",
    "# ])\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Grayscale(3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((32, 32)), \n",
    "            \n",
    "            transforms.Normalize(0.5, 0.5),\n",
    "        ])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
    "\n",
    "pil_dataset = DataClass(split='train', download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = torch_data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader_at_eval = torch_data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch_data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 32, 32])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEST CE Result for BMNIST below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 000  Train Loss : 0.96551  Val Loss : 0.71291   BatchID= 34   Val_AUC=0.6009   Best_Val_AUC=0.6009\n",
      "Epoch : 005  Train Loss : 0.46612  Val Loss : 0.53674   BatchID= 34   Val_AUC=0.7830   Best_Val_AUC=0.7830\n",
      "Epoch : 010  Train Loss : 0.32923  Val Loss : 0.45366   BatchID= 34   Val_AUC=0.7993   Best_Val_AUC=0.7993\n",
      "Epoch : 015  Train Loss : 0.27267  Val Loss : 0.44311   BatchID= 34   Val_AUC=0.8248   Best_Val_AUC=0.8248\n",
      "Epoch : 020  Train Loss : 0.24289  Val Loss : 0.41883   BatchID= 34   Val_AUC=0.8331   Best_Val_AUC=0.8331\n",
      "Epoch : 025  Train Loss : 0.20464  Val Loss : 0.41020   BatchID= 34   Val_AUC=0.8348   Best_Val_AUC=0.8348\n",
      "Epoch : 030  Train Loss : 0.18576  Val Loss : 0.37117   BatchID= 34   Val_AUC=0.8707   Best_Val_AUC=0.8707\n",
      "Epoch : 035  Train Loss : 0.16647  Val Loss : 0.36983   BatchID= 34   Val_AUC=0.8761   Best_Val_AUC=0.8761\n",
      "Epoch : 040  Train Loss : 0.15724  Val Loss : 0.38295   BatchID= 34   Val_AUC=0.8860   Best_Val_AUC=0.8860\n",
      "Epoch : 045  Train Loss : 0.13732  Val Loss : 0.37432   BatchID= 34   Val_AUC=0.8947   Best_Val_AUC=0.8947\n",
      "Epoch : 050  Train Loss : 0.10326  Val Loss : 0.39088   BatchID= 34   Val_AUC=0.9048   Best_Val_AUC=0.9048\n",
      "Epoch : 055  Train Loss : 0.11677  Val Loss : 0.43178   BatchID= 34   Val_AUC=0.8722   Best_Val_AUC=0.9048\n",
      "Epoch : 060  Train Loss : 0.14472  Val Loss : 0.37687   BatchID= 34   Val_AUC=0.8931   Best_Val_AUC=0.9048\n",
      "Epoch : 065  Train Loss : 0.14968  Val Loss : 0.39488   BatchID= 34   Val_AUC=0.8709   Best_Val_AUC=0.9048\n",
      "Epoch : 070  Train Loss : 0.12034  Val Loss : 0.41030   BatchID= 34   Val_AUC=0.8661   Best_Val_AUC=0.9048\n",
      "Epoch : 075  Train Loss : 0.18711  Val Loss : 0.42389   BatchID= 34   Val_AUC=0.8816   Best_Val_AUC=0.9048\n",
      "Epoch : 080  Train Loss : 0.13982  Val Loss : 0.39636   BatchID= 34   Val_AUC=0.8881   Best_Val_AUC=0.9048\n",
      "Epoch : 085  Train Loss : 0.11383  Val Loss : 0.40294   BatchID= 34   Val_AUC=0.8893   Best_Val_AUC=0.9048\n",
      "Epoch : 090  Train Loss : 0.19614  Val Loss : 0.41582   BatchID= 34   Val_AUC=0.8906   Best_Val_AUC=0.9048\n",
      "Epoch : 095  Train Loss : 0.20886  Val Loss : 0.50113   BatchID= 34   Val_AUC=0.8607   Best_Val_AUC=0.9048\n",
      "Epoch : 100  Train Loss : 0.14946  Val Loss : 0.42799   BatchID= 34   Val_AUC=0.8920   Best_Val_AUC=0.9048\n",
      "Epoch : 105  Train Loss : 0.09769  Val Loss : 0.38655   BatchID= 34   Val_AUC=0.8868   Best_Val_AUC=0.9048\n",
      "Epoch : 110  Train Loss : 0.12000  Val Loss : 0.43462   BatchID= 34   Val_AUC=0.8964   Best_Val_AUC=0.9048\n",
      "Epoch : 115  Train Loss : 0.14339  Val Loss : 0.41925   BatchID= 34   Val_AUC=0.8841   Best_Val_AUC=0.9048\n",
      "Epoch : 120  Train Loss : 0.11580  Val Loss : 0.38729   BatchID= 34   Val_AUC=0.8914   Best_Val_AUC=0.9048\n",
      "Epoch : 125  Train Loss : 0.12669  Val Loss : 0.41810   BatchID= 34   Val_AUC=0.8968   Best_Val_AUC=0.9048\n",
      "Epoch : 130  Train Loss : 0.16506  Val Loss : 0.39842   BatchID= 34   Val_AUC=0.9058   Best_Val_AUC=0.9058\n",
      "Epoch : 135  Train Loss : 0.12886  Val Loss : 0.44095   BatchID= 34   Val_AUC=0.8592   Best_Val_AUC=0.9058\n",
      "Epoch : 140  Train Loss : 0.09915  Val Loss : 0.44310   BatchID= 34   Val_AUC=0.8605   Best_Val_AUC=0.9058\n",
      "Epoch : 145  Train Loss : 0.09656  Val Loss : 0.46625   BatchID= 34   Val_AUC=0.8799   Best_Val_AUC=0.9058\n",
      "Epoch : 150  Train Loss : 0.09806  Val Loss : 0.41866   BatchID= 34   Val_AUC=0.9058   Best_Val_AUC=0.9058\n",
      "Epoch : 155  Train Loss : 0.14725  Val Loss : 0.43437   BatchID= 34   Val_AUC=0.8843   Best_Val_AUC=0.9058\n",
      "Epoch : 160  Train Loss : 0.14966  Val Loss : 0.42991   BatchID= 34   Val_AUC=0.8868   Best_Val_AUC=0.9058\n",
      "Epoch : 165  Train Loss : 0.22396  Val Loss : 0.56847   BatchID= 34   Val_AUC=0.8377   Best_Val_AUC=0.9058\n",
      "Epoch : 170  Train Loss : 0.10036  Val Loss : 0.49523   BatchID= 34   Val_AUC=0.8590   Best_Val_AUC=0.9058\n",
      "Epoch : 175  Train Loss : 0.12426  Val Loss : 0.51992   BatchID= 34   Val_AUC=0.8511   Best_Val_AUC=0.9058\n",
      "Epoch : 180  Train Loss : 0.14632  Val Loss : 0.51895   BatchID= 34   Val_AUC=0.8607   Best_Val_AUC=0.9058\n",
      "Epoch : 185  Train Loss : 0.12226  Val Loss : 0.45036   BatchID= 34   Val_AUC=0.8728   Best_Val_AUC=0.9058\n",
      "Epoch : 190  Train Loss : 0.13693  Val Loss : 0.46044   BatchID= 34   Val_AUC=0.8741   Best_Val_AUC=0.9058\n",
      "Epoch : 195  Train Loss : 0.12458  Val Loss : 0.59878   BatchID= 34   Val_AUC=0.8429   Best_Val_AUC=0.9058\n",
      "Epoch : 200  Train Loss : 0.06607  Val Loss : 0.48838   BatchID= 34   Val_AUC=0.8605   Best_Val_AUC=0.9058\n"
     ]
    }
   ],
   "source": [
    "from libauc.models import resnet18\n",
    "from libauc.losses import AUCMLoss\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torch_data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "\n",
    "\n",
    "from libauc.losses import AUCMLoss, CrossEntropyLoss\n",
    "from libauc.optimizers import PESG, Adam\n",
    "from libauc.models import densenet121 as DenseNet121\n",
    "from libauc.datasets import CheXpert\n",
    "\n",
    "import torch \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "\n",
    "data_flag = 'breastmnist'\n",
    "download = True\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "lr = 0.001\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "# Data Transformation\n",
    "data_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Grayscale(3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((32, 32)), \n",
    "            \n",
    "            transforms.Normalize(0.5, 0.5),\n",
    "        ])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
    "\n",
    "pil_dataset = DataClass(split='train', download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = torch_data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader_at_eval = torch_data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch_data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# BEST CROSS ENTROPY \n",
    "lr = 1e-5\n",
    "weight_decay = 2e-5\n",
    "model = resnet18(num_classes=2)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# training\n",
    "best_val_auc = 0 \n",
    "for epoch in range(201):\n",
    "    train_losses = []\n",
    "    for idx, data in enumerate(train_loader):\n",
    "      train_data, train_labels = data\n",
    "      train_data, train_labels  = train_data.cuda(), train_labels.cuda()\n",
    "      y_pred = model(train_data)\n",
    "      loss = criterion(y_pred, train_labels.squeeze().type(torch.LongTensor).cuda())\n",
    "      train_losses.append(loss.item())\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    if epoch%5==0:\n",
    "      print(\"Epoch : {:03d}  Train Loss : {:.5f}  \".format(epoch, np.mean(train_losses)), end='')\n",
    "      model.eval()\n",
    "      with torch.no_grad():    \n",
    "          test_pred = []\n",
    "          test_true = [] \n",
    "          test_losses = []\n",
    "          for jdx, data in enumerate(test_loader):\n",
    "              test_data, test_labels = data\n",
    "              test_data = test_data.cuda()\n",
    "              y_pred = model(test_data)\n",
    "              test_pred.append(y_pred.cpu().detach().numpy())\n",
    "              test_true.append(test_labels.numpy())\n",
    "              test_losses.append(criterion(y_pred, test_labels.squeeze().type(torch.LongTensor).cuda()).item())\n",
    "\n",
    "          test_true = np.concatenate(test_true)\n",
    "          test_pred = np.concatenate(test_pred)\n",
    "          val_auc_mean =  roc_auc_score(test_true.squeeze(), test_pred[:,1]) \n",
    "          print(\"Val Loss : {:.5f}   \".format(np.mean(test_losses)), end='')\n",
    "          model.train()\n",
    "\n",
    "          if best_val_auc < val_auc_mean:\n",
    "              best_val_auc = val_auc_mean\n",
    "              torch.save(model.state_dict(), 'ce_pretrained_model.pth')\n",
    "\n",
    "          print ('BatchID= {}   Val_AUC={:.4f}   Best_Val_AUC={:.4f}'.format(\n",
    "              idx, val_auc_mean, best_val_auc ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 000  Train Loss : 0.05986 Val Loss : 1.08475   Val CE Loss : 0.66030  BatchID= 34   Val_AUC=0.7715   Best_Val_AUC=0.8079\n",
      "Epoch : 005  Train Loss : 0.05511 Val Loss : 1.30268   Val CE Loss : 0.67616  BatchID= 34   Val_AUC=0.7575   Best_Val_AUC=0.8079\n",
      "Epoch : 010  Train Loss : 0.05451 Val Loss : 0.95586   Val CE Loss : 0.69618  BatchID= 34   Val_AUC=0.7594   Best_Val_AUC=0.8079\n",
      "Epoch : 015  Train Loss : 0.05996 Val Loss : 0.80668   Val CE Loss : 0.69962  BatchID= 34   Val_AUC=0.7907   Best_Val_AUC=0.8079\n",
      "Epoch : 020  Train Loss : 0.06183 Val Loss : 1.06070   Val CE Loss : 0.73022  BatchID= 34   Val_AUC=0.7680   Best_Val_AUC=0.8079\n",
      "Epoch : 025  Train Loss : 0.05332 Val Loss : 0.94242   Val CE Loss : 0.67984  BatchID= 34   Val_AUC=0.7701   Best_Val_AUC=0.8079\n",
      "Epoch : 030  Train Loss : 0.05779 Val Loss : 1.07961   Val CE Loss : 0.67343  BatchID= 34   Val_AUC=0.7650   Best_Val_AUC=0.8079\n",
      "Epoch : 035  Train Loss : 0.06424 Val Loss : 0.26654   Val CE Loss : 0.68599  BatchID= 34   Val_AUC=0.7592   Best_Val_AUC=0.8079\n",
      "Epoch : 040  Train Loss : 0.06768 Val Loss : 0.76495   Val CE Loss : 0.67821  BatchID= 34   Val_AUC=0.7446   Best_Val_AUC=0.8079\n",
      "Epoch : 045  Train Loss : 0.06949 Val Loss : 0.77425   Val CE Loss : 0.66188  BatchID= 34   Val_AUC=0.7646   Best_Val_AUC=0.8079\n",
      "Epoch : 050  Train Loss : 0.06998 Val Loss : 0.89913   Val CE Loss : 0.68138  BatchID= 34   Val_AUC=0.7467   Best_Val_AUC=0.8079\n",
      "Epoch : 055  Train Loss : 0.06869 Val Loss : 0.58029   Val CE Loss : 0.72636  BatchID= 34   Val_AUC=0.7682   Best_Val_AUC=0.8079\n",
      "Epoch : 060  Train Loss : 0.06774 Val Loss : 0.92981   Val CE Loss : 0.70667  BatchID= 34   Val_AUC=0.7565   Best_Val_AUC=0.8079\n",
      "Epoch : 065  Train Loss : 0.06403 Val Loss : 0.82021   Val CE Loss : 0.71115  BatchID= 34   Val_AUC=0.7594   Best_Val_AUC=0.8079\n",
      "Epoch : 070  Train Loss : 0.06205 Val Loss : 0.86008   Val CE Loss : 0.71560  BatchID= 34   Val_AUC=0.7761   Best_Val_AUC=0.8079\n",
      "Epoch : 075  Train Loss : 0.06431 Val Loss : 0.79173   Val CE Loss : 0.69966  BatchID= 34   Val_AUC=0.7387   Best_Val_AUC=0.8079\n",
      "Epoch : 080  Train Loss : 0.06298 Val Loss : 0.68202   Val CE Loss : 0.73507  BatchID= 34   Val_AUC=0.7730   Best_Val_AUC=0.8079\n",
      "Epoch : 085  Train Loss : 0.06241 Val Loss : 0.70258   Val CE Loss : 0.74912  BatchID= 34   Val_AUC=0.7874   Best_Val_AUC=0.8079\n",
      "Epoch : 090  Train Loss : 0.06708 Val Loss : 0.66256   Val CE Loss : 0.73343  BatchID= 34   Val_AUC=0.7625   Best_Val_AUC=0.8079\n",
      "Epoch : 095  Train Loss : 0.06416 Val Loss : 0.26899   Val CE Loss : 0.69896  BatchID= 34   Val_AUC=0.7794   Best_Val_AUC=0.8079\n",
      "Epoch : 100  Train Loss : 0.06318 Val Loss : 0.47611   Val CE Loss : 0.70748  BatchID= 34   Val_AUC=0.7746   Best_Val_AUC=0.8079\n",
      "Epoch : 105  Train Loss : 0.06939 Val Loss : 0.74631   Val CE Loss : 0.70809  BatchID= 34   Val_AUC=0.7500   Best_Val_AUC=0.8079\n",
      "Epoch : 110  Train Loss : 0.06122 Val Loss : 1.33651   Val CE Loss : 0.69909  BatchID= 34   Val_AUC=0.7431   Best_Val_AUC=0.8079\n",
      "Epoch : 115  Train Loss : 0.05806 Val Loss : 0.83818   Val CE Loss : 0.74465  BatchID= 34   Val_AUC=0.7663   Best_Val_AUC=0.8079\n",
      "Epoch : 120  Train Loss : 0.05902 Val Loss : 1.93681   Val CE Loss : 0.72814  BatchID= 34   Val_AUC=0.7498   Best_Val_AUC=0.8079\n",
      "Epoch : 125  Train Loss : 0.06018 Val Loss : 0.97302   Val CE Loss : 0.72206  BatchID= 34   Val_AUC=0.7847   Best_Val_AUC=0.8079\n",
      "Epoch : 130  Train Loss : 0.06061 Val Loss : 1.05451   Val CE Loss : 0.68850  BatchID= 34   Val_AUC=0.7794   Best_Val_AUC=0.8079\n",
      "Epoch : 135  Train Loss : 0.06055 Val Loss : 1.07660   Val CE Loss : 0.61179  BatchID= 34   Val_AUC=0.7525   Best_Val_AUC=0.8079\n",
      "Epoch : 140  Train Loss : 0.05454 Val Loss : 0.77621   Val CE Loss : 0.68109  BatchID= 34   Val_AUC=0.7882   Best_Val_AUC=0.8079\n",
      "Epoch : 145  Train Loss : 0.05298 Val Loss : 0.79848   Val CE Loss : 0.68618  BatchID= 34   Val_AUC=0.7707   Best_Val_AUC=0.8079\n",
      "Epoch : 150  Train Loss : 0.06080 Val Loss : 1.10832   Val CE Loss : 0.70670  BatchID= 34   Val_AUC=0.7715   Best_Val_AUC=0.8079\n",
      "Epoch : 155  Train Loss : 0.06386 Val Loss : 1.30078   Val CE Loss : 0.72896  BatchID= 34   Val_AUC=0.7744   Best_Val_AUC=0.8079\n",
      "Epoch : 160  Train Loss : 0.05513 Val Loss : 0.99229   Val CE Loss : 0.70003  BatchID= 34   Val_AUC=0.7878   Best_Val_AUC=0.8079\n",
      "Epoch : 165  Train Loss : 0.05137 Val Loss : 1.08180   Val CE Loss : 0.70157  BatchID= 34   Val_AUC=0.7769   Best_Val_AUC=0.8079\n",
      "Epoch : 170  Train Loss : 0.06284 Val Loss : 1.02501   Val CE Loss : 0.76353  BatchID= 34   Val_AUC=0.7799   Best_Val_AUC=0.8079\n",
      "Epoch : 175  Train Loss : 0.06471 Val Loss : 0.91520   Val CE Loss : 0.75829  BatchID= 34   Val_AUC=0.7882   Best_Val_AUC=0.8079\n",
      "Epoch : 180  Train Loss : 0.05570 Val Loss : 0.46578   Val CE Loss : 0.78317  BatchID= 34   Val_AUC=0.8070   Best_Val_AUC=0.8079\n",
      "Epoch : 185  Train Loss : 0.05671 Val Loss : 1.16723   Val CE Loss : 0.77838  BatchID= 34   Val_AUC=0.7976   Best_Val_AUC=0.8079\n",
      "Epoch : 190  Train Loss : 0.06238 Val Loss : 0.68102   Val CE Loss : 0.73071  BatchID= 34   Val_AUC=0.7922   Best_Val_AUC=0.8079\n",
      "Epoch : 195  Train Loss : 0.05729 Val Loss : 0.73887   Val CE Loss : 0.73309  BatchID= 34   Val_AUC=0.8018   Best_Val_AUC=0.8079\n",
      "Epoch : 200  Train Loss : 0.05583 Val Loss : 1.06059   Val CE Loss : 0.71583  BatchID= 34   Val_AUC=0.7832   Best_Val_AUC=0.8079\n"
     ]
    }
   ],
   "source": [
    "# # # preprocessing\n",
    "# # data_transform = transforms.Compose([\n",
    "# #     transforms.ToTensor(),\n",
    "# #     # transforms.Grayscale(3),\n",
    "# #     transforms.Resize((28, 28)), \n",
    "# #     transforms.Normalize(mean=[.5], std=[.5])\n",
    "# # ])\n",
    "\n",
    "# train_transform = transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.ToPILImage(),\n",
    "#             transforms.Grayscale(3),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Resize((32, 32)),  \n",
    "#             transforms.RandomHorizontalFlip(0.3),\n",
    "#             transforms.RandomVerticalFlip(0.3),\n",
    "#             # transforms.RandomEqualize(0.3),    \n",
    "#             # transforms.RandomErasing([p, scale, ratio, value, inplace]),\n",
    "#             # transforms.AutoAugment(),               \n",
    "#             # transforms.ElasticTransform(),\n",
    "#             transforms.RandomRotation(30),\n",
    "#             transforms.Normalize(0.5, 0.5),\n",
    "#         ])\n",
    "\n",
    "# test_transform = transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.ToPILImage(),\n",
    "#             transforms.Grayscale(3),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Resize((32, 32)),             \n",
    "#             transforms.Normalize(0.5, 0.5),\n",
    "#         ])\n",
    "\n",
    "# # load the data\n",
    "# train_dataset = DataClass(split='train', transform=train_transform, download=download)\n",
    "# test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
    "\n",
    "# pil_dataset = DataClass(split='train', download=download)\n",
    "\n",
    "# # encapsulate data into dataloader form\n",
    "# train_loader = torch_data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader_at_eval = torch_data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "# test_loader = torch_data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# lr = 0.01 # using smaller learning rate is better\n",
    "# epoch_decay = 2e-5\n",
    "# weight_decay = 1e-7\n",
    "# margin = 1.0\n",
    "\n",
    "# model = resnet18(num_classes=2)\n",
    "# model = model.cuda()\n",
    "# # criterion = nn.CrossEntropyLoss()  \n",
    "# # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# criterion = AUCMLoss()\n",
    "# optimizer = PESG(model, \n",
    "#                  loss_fn=criterion, \n",
    "#                  lr=lr, \n",
    "#                  margin=margin, \n",
    "#                  epoch_decay=epoch_decay, \n",
    "#                  weight_decay=weight_decay)\n",
    "# CE_loss = nn.CrossEntropyLoss()\n",
    "# # training\n",
    "# best_val_auc = 0 \n",
    "for epoch in range(201):\n",
    "    train_losses = []\n",
    "    for idx, data in enumerate(train_loader):\n",
    "      train_data, train_labels = data\n",
    "      train_data, train_labels  = train_data.cuda(), train_labels.cuda()\n",
    "      y_pred = model(train_data)\n",
    "      y_pred = torch.sigmoid(y_pred)\n",
    "      loss = criterion(y_pred, train_labels.squeeze().type(torch.LongTensor).cuda())\n",
    "      train_losses.append(loss.item()/len(data))\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "        \n",
    "      # validation  \n",
    "    #   if idx % 10 == 0:\n",
    "    \n",
    "    if epoch%5==0:\n",
    "      print(\"Epoch : {:03d}  Train Loss : {:.5f} \".format(epoch, np.mean(train_losses)), end='')\n",
    "      model.eval()\n",
    "      with torch.no_grad():    \n",
    "          test_pred = []\n",
    "          test_true = [] \n",
    "          test_losses = []\n",
    "          test_CE_losses = []\n",
    "          for jdx, data in enumerate(test_loader):\n",
    "              test_data, test_labels = data\n",
    "              test_data = test_data.cuda()\n",
    "              y_pred = model(test_data)\n",
    "              test_pred.append(y_pred.cpu().detach().numpy())\n",
    "              test_true.append(test_labels.numpy())\n",
    "              test_losses.append(criterion(y_pred, test_labels.squeeze().type(torch.LongTensor).cuda()).item() / len(data))\n",
    "              test_CE_losses.append(CE_loss(y_pred, test_labels.squeeze().type(torch.LongTensor).cuda()).cpu())\n",
    "\n",
    "          test_true = np.concatenate(test_true)\n",
    "          test_pred = np.concatenate(test_pred)\n",
    "          val_auc_mean =  roc_auc_score(test_true.squeeze(), test_pred[:,1]) \n",
    "          print(\"Val Loss : {:.5f}   Val CE Loss : {:.5f}  \".format(np.mean(test_losses), np.mean(test_CE_losses)), end = '')\n",
    "          model.train()\n",
    "\n",
    "          if best_val_auc < val_auc_mean:\n",
    "              best_val_auc = val_auc_mean\n",
    "              torch.save(model.state_dict(), 'ce_pretrained_model.pth')\n",
    "\n",
    "          print ('BatchID= {}   Val_AUC={:.4f}   Best_Val_AUC={:.4f}'.format(\n",
    "              idx, val_auc_mean, best_val_auc ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7321, device='cuda:0')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 =nn.CrossEntropyLoss()\n",
    "l1(y_pred, test_labels.squeeze().type(torch.LongTensor).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        0, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.squeeze().type(torch.LongTensor).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1083, -2.4466],\n",
       "        [-1.4856, -1.3008],\n",
       "        [-0.0136, -0.7149],\n",
       "        [-1.7003, -1.8327],\n",
       "        [-1.8125, -1.2051],\n",
       "        [-1.6341, -1.0869],\n",
       "        [-2.0423, -2.1351],\n",
       "        [-1.6785, -1.3222],\n",
       "        [-1.9418, -1.4862],\n",
       "        [-0.7217, -0.9000],\n",
       "        [-2.1665, -1.4439],\n",
       "        [-1.4988, -1.8446],\n",
       "        [-1.3270, -1.8724],\n",
       "        [-1.4483, -1.5633],\n",
       "        [-0.6035, -1.0628],\n",
       "        [-1.3239, -1.2100],\n",
       "        [-1.4539, -1.3997],\n",
       "        [-1.9640, -1.4416],\n",
       "        [-1.2585, -1.1549],\n",
       "        [-1.2373, -1.3626],\n",
       "        [-2.0416, -2.3888],\n",
       "        [-1.2686, -1.4588],\n",
       "        [-0.5264, -0.5442],\n",
       "        [-1.4517, -1.4630],\n",
       "        [-2.0114, -1.4631],\n",
       "        [-1.6009, -1.5631],\n",
       "        [-1.7191, -1.2959],\n",
       "        [-1.4363, -1.4959],\n",
       "        [-1.4650, -1.6425],\n",
       "        [-0.0229, -0.5606],\n",
       "        [-1.1428, -1.1456],\n",
       "        [-1.2979, -1.2396]], device='cuda:0')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m target\n",
      "\u001b[1;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BasicBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (1): BasicBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isr_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
